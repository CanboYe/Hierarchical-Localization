{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1207)\n",
      "(2,)\n",
      "(1207, 2)\n",
      "(1207,)\n",
      "[1462.9375  579.8125]\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/home/cloudlet/work/Hierarchical-Localization/outputs/argo_sample_2'\n",
    "feature_path = Path(output_dir, 'feats-superpoint-n4096-r1024.h5')\n",
    "assert feature_path.exists(), feature_path\n",
    "feature_file = h5py.File(str(feature_path), 'r')\n",
    "# print(feature_file.keys())\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['descriptors'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['image_size'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['keypoints'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['scores'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['keypoints'][666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ring_side_left_315978406032855224.jpg_ring_front_center_315978414557655080.jpg\n",
      "(1881,)\n",
      "(1881,)\n"
     ]
    }
   ],
   "source": [
    "match_path = Path(output_dir, 'feats-superpoint-n4096-r1024_matches-superglue_pairs60_argo_query.h5')\n",
    "assert match_path.exists(), match_path\n",
    "match_file = h5py.File(str(match_path), 'r')\n",
    "print(list(match_file.keys())[0])\n",
    "print(match_file['ring_side_left_315978406032855224.jpg_ring_front_center_315978414557655080.jpg']['matches0'].shape)\n",
    "print(match_file['ring_side_left_315978406032855224.jpg_ring_front_center_315978414557655080.jpg']['matching_scores0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "result_path = Path(output_dir, 'argo_hloc_superpoint+superglue_netvlad60.txt_logs.pkl')\n",
    "with open(str(result_path), 'rb') as f:\n",
    "    logs = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features', 'matches', 'retrieval', 'loc'])\n",
      "ring_side_left_315978415256957824.jpg\n"
     ]
    }
   ],
   "source": [
    "print(logs.keys())\n",
    "query = list(logs['loc'].keys())[0]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['db', 'PnP_ret', 'keypoints_query', 'points3D_xyz', 'points3D_ids', 'num_matches', 'keypoint_index_to_db'])\n"
     ]
    }
   ],
   "source": [
    "loc = logs['loc'][query]\n",
    "print(loc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.read_write_model import read_images_binary, read_points3d_binary\n",
    "sfm_model = Path(output_dir, 'sfm_superpoint+superglue+netvlad/models/model_aligned')\n",
    "images = read_images_binary(sfm_model / 'images.bin')\n",
    "points3D = read_points3d_binary(sfm_model / 'points3D.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n",
      "110001\n"
     ]
    }
   ],
   "source": [
    "print(len(list(images.keys())))\n",
    "print(len(list(points3D.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NetVLAD to get the TOP 20 pairs for a query image\n",
    "TODO: fix the cuda version conflicts between tf and pytorch\n",
    "TODO: fix the environment conflicts between hfnet and hloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-173c1411f717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetvlad_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m image_batch = tf.placeholder(\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import netvlad_tf.net_from_mat as nfm\n",
    "import netvlad_tf.nets as nets\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "image_batch = tf.placeholder(\n",
    "        dtype=tf.float32, shape=[None, None, None, 3])\n",
    "\n",
    "net_out = nets.vgg16NetvladPca(image_batch)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, nets.defaultCheckpoint())\n",
    "\n",
    "inim = cv2.imread(nfm.exampleImgPath())\n",
    "inim = cv2.cvtColor(inim, cv2.COLOR_BGR2RGB)\n",
    "print(inim.shape)\n",
    "batch = np.expand_dims(inim, axis=0)\n",
    "print(batch.shape)\n",
    "result = sess.run(net_out, feed_dict={image_batch: batch})\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB images\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "result_list = []\n",
    "filename_list = []\n",
    "dataset_path = '/home/cloudlet/datasets/argoverse-tracking/sample/test_1017_2/'\n",
    "for filename in tqdm(glob.glob(dataset_path + 'db/*.jpg')):\n",
    "    filename_list.append(filename.split('/')[-1])\n",
    "    inim = cv2.imread(filename)\n",
    "    inim = cv2.cvtColor(inim, cv2.COLOR_BGR2RGB)\n",
    "#     print(inim.shape)\n",
    "    batch = np.expand_dims(inim, axis=0)\n",
    "#     print(batch.shape)\n",
    "    result = sess.run(net_out, feed_dict={image_batch: batch})\n",
    "    result_list.append(result)\n",
    "print(len(filename_list))\n",
    "db = np.concatenate(result_list, axis=0)\n",
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query image\n",
    "base_dir = '/home/cloudlet/work/Hierarchical-Localization/test_query/'\n",
    "result_list_query = []\n",
    "filename_list_query = []\n",
    "for filename in tqdm(glob.glob(base_dir + '*.jpg')):\n",
    "    filename_list_query.append(filename.split('/')[-1])\n",
    "    inim = cv2.imread(filename)\n",
    "    inim = cv2.cvtColor(inim, cv2.COLOR_BGR2RGB)\n",
    "#     print(inim.shape)\n",
    "    batch = np.expand_dims(inim, axis=0)\n",
    "#     print(batch.shape)\n",
    "    result = sess.run(net_out, feed_dict={image_batch: batch})\n",
    "    result_list_query.append(result)\n",
    "print(filename_list_query)\n",
    "print(len(filename_list_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(desc1, desc2):\n",
    "    # For normalized descriptors, computing the distance is a simple matrix multiplication.\n",
    "    return 2 * (1 - desc1 @ desc2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_path = base_dir + 'pairs'\n",
    "top_k = 20\n",
    "import os\n",
    "if not os.path.exists(pair_path):\n",
    "    os.makedirs(pair_path)\n",
    "with open(pair_path + 'pairs'+str(top_k)+'_argo_query.txt', 'w') as file:\n",
    "    for i1, query in enumerate(result_list_query):\n",
    "        query = query.flatten()\n",
    "        dist = compute_distance(query, db)\n",
    "        index_min_k = np.argsort(dist)[:top_k]\n",
    "\n",
    "        for i2, db_file_index in enumerate(index_min_k):\n",
    "            if i2 == len(index_min_k)-1 and i1 == len(result_list_query)-1:\n",
    "                line = filename_list_query[i1] + ' ' + filename_list[db_file_index]\n",
    "            else:\n",
    "                line = filename_list_query[i1] + ' ' + filename_list[db_file_index] + '\\n'\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, visualization\n",
    "dataset = Path('/home/cloudlet/datasets/argoverse-tracking/sample/test_1017_2')\n",
    "images_db = dataset / 'db'\n",
    "images_query = Path('test_query/query_image')\n",
    "\n",
    "pairs = Path('test_query/pairs/')\n",
    "loc_pairs = pairs / 'pairs20_argo_query.txt'  # top 20 retrieved by NetVLAD\n",
    "\n",
    "outputs = Path('test_query/outputs/')\n",
    "sfm_dir =  Path('outputs/argo_sample_2/sfm_superpoint+superglue+netvlad')\n",
    "\n",
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']\n",
    "\n",
    "features = feature_conf['output']\n",
    "feature_file = f\"{features}.h5\"\n",
    "match_file_query = f\"{features}_{matcher_conf['output']}_{loc_pairs.stem}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:31:07 INFO] Extracting local features with configuration:\n",
      "{'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 3},\n",
      " 'output': 'feats-superpoint-n4096-r1024',\n",
      " 'preprocessing': {'grayscale': True, 'resize_max': 1024}}\n",
      "Loaded SuperPoint model\n",
      "[10/28/2020 00:31:07 INFO] Found 785 images in root /home/cloudlet/datasets/argoverse-tracking/sample/test_1017_2/db.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 785/785 [00:12<00:00, 63.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:31:19 INFO] Finished exporting features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features.main(feature_conf, images_db, outputs/'db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:31:19 INFO] Extracting local features with configuration:\n",
      "{'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 3},\n",
      " 'output': 'feats-superpoint-n4096-r1024',\n",
      " 'preprocessing': {'grayscale': True, 'resize_max': 1024}}\n",
      "Loaded SuperPoint model\n",
      "[10/28/2020 00:31:19 INFO] Found 1 images in root test_query/query_image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:31:19 INFO] Finished exporting features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features.main(feature_conf, images_query, outputs/'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match the query image with db images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "\n",
    "from hloc import matchers\n",
    "from hloc.utils.base_model import dynamic_load\n",
    "from hloc.utils.parsers import names_to_pair\n",
    "\n",
    "\n",
    "'''\n",
    "A set of standard configurations that can be directly selected from the command\n",
    "line using their name. Each is a dictionary with the following entries:\n",
    "    - output: the name of the match file that will be generated.\n",
    "    - model: the model configuration, as passed to a feature matcher.\n",
    "'''\n",
    "confs = {\n",
    "    'superglue': {\n",
    "        'output': 'matches-superglue',\n",
    "        'model': {\n",
    "            'name': 'superglue',\n",
    "            'weights': 'outdoor',\n",
    "            'sinkhorn_iterations': 50,\n",
    "        },\n",
    "    },\n",
    "    'NN': {\n",
    "        'output': 'matches-NN-mutual-dist.7',\n",
    "        'model': {\n",
    "            'name': 'nearest_neighbor',\n",
    "            'mutual_check': True,\n",
    "            'distance_threshold': 0.7,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def matcher(conf, pairs, features, export_dir):\n",
    "    logging.info('Matching local features with configuration:'\n",
    "                 f'\\n{pprint.pformat(conf)}')\n",
    "\n",
    "    feature_db_path = Path(export_dir, 'db/'+features+'.h5')\n",
    "    assert feature_db_path.exists(), feature_db_path\n",
    "    feature_db_file = h5py.File(str(feature_db_path), 'r')\n",
    "\n",
    "    feature_query_path = Path(export_dir, 'query/'+features+'.h5')\n",
    "    assert feature_query_path.exists(), feature_query_path\n",
    "    feature_query_file = h5py.File(str(feature_query_path), 'r')\n",
    "\n",
    "    pairs_name = pairs.stem\n",
    "    assert pairs.exists(), pairs\n",
    "    with open(pairs, 'r') as f:\n",
    "        pair_list = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    Model = dynamic_load(matchers, conf['model']['name'])\n",
    "    model = Model(conf['model']).eval().to(device)\n",
    "\n",
    "    match_name = f'{features}_{conf[\"output\"]}_{pairs_name}'\n",
    "    match_path = Path(export_dir, match_name+'.h5')\n",
    "    match_file = h5py.File(str(match_path), 'a')\n",
    "\n",
    "    matched = set()\n",
    "    for pair in tqdm(pair_list, smoothing=.1):\n",
    "        name0, name1 = pair.split(' ')\n",
    "        pair = names_to_pair(name0, name1)\n",
    "\n",
    "        # Avoid to recompute duplicates to save time\n",
    "        if len({(name0, name1), (name1, name0)} & matched) \\\n",
    "                or pair in match_file:\n",
    "            continue\n",
    "\n",
    "        data = {}\n",
    "        feats0, feats1 = feature_query_file[name0], feature_db_file[name1]\n",
    "        for k in feats1.keys():\n",
    "            data[k+'0'] = feats0[k].__array__()\n",
    "        for k in feats1.keys():\n",
    "            data[k+'1'] = feats1[k].__array__()\n",
    "        data = {k: torch.from_numpy(v)[None].float().to(device)\n",
    "                for k, v in data.items()}\n",
    "\n",
    "        # some matchers might expect an image but only use its size\n",
    "        data['image0'] = torch.empty((1, 1,)+tuple(feats0['image_size'])[::-1])\n",
    "        data['image1'] = torch.empty((1, 1,)+tuple(feats1['image_size'])[::-1])\n",
    "\n",
    "        pred = model(data)\n",
    "        grp = match_file.create_group(pair)\n",
    "        matches = pred['matches0'][0].cpu().short().numpy()\n",
    "        grp.create_dataset('matches0', data=matches)\n",
    "\n",
    "        if 'matching_scores0' in pred:\n",
    "            scores = pred['matching_scores0'][0].cpu().half().numpy()\n",
    "            grp.create_dataset('matching_scores0', data=scores)\n",
    "\n",
    "        matched |= {(name0, name1), (name1, name0)}\n",
    "\n",
    "    match_file.close()\n",
    "    logging.info('Finished exporting matches.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:46:01 INFO] Matching local features with configuration:\n",
      "{'model': {'name': 'superglue',\n",
      "           'sinkhorn_iterations': 50,\n",
      "           'weights': 'outdoor'},\n",
      " 'output': 'matches-superglue'}\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:02,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/28/2020 00:46:03 INFO] Finished exporting matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matcher(matcher_conf, loc_pairs, features, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the 3D points given Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n",
      "(256, 1207)\n",
      "(2,)\n",
      "(1207, 2)\n",
      "(1207,)\n"
     ]
    }
   ],
   "source": [
    "feature_path = Path(outputs, 'db/feats-superpoint-n4096-r1024.h5')\n",
    "assert feature_path.exists(), feature_path\n",
    "feature_file = h5py.File(str(feature_path), 'r')\n",
    "print(len(list(feature_file.keys())))\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['descriptors'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['image_size'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['keypoints'].shape)\n",
    "print(feature_file['ring_front_center_315978406032859416.jpg']['scores'].shape)\n",
    "# print(feature_file['ring_front_center_315978406032859416.jpg']['keypoints'][666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(256, 1508)\n",
      "(2,)\n",
      "(1508, 2)\n",
      "(1508,)\n"
     ]
    }
   ],
   "source": [
    "feature_path_q = Path(outputs, 'query/feats-superpoint-n4096-r1024.h5')\n",
    "assert feature_path_q.exists(), feature_path_q\n",
    "feature_file_q = h5py.File(str(feature_path_q), 'r')\n",
    "print(len(list(feature_file_q.keys())))\n",
    "print(feature_file_q['query.jpg']['descriptors'].shape)\n",
    "print(feature_file_q['query.jpg']['image_size'].shape)\n",
    "print(feature_file_q['query.jpg']['keypoints'].shape)\n",
    "print(feature_file_q['query.jpg']['scores'].shape)\n",
    "# print(feature_file['ring_front_center_315978406032859416.jpg']['keypoints'][666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.jpg_ring_front_center_315978410361853528.jpg\n",
      "(1508,)\n",
      "(1508,)\n"
     ]
    }
   ],
   "source": [
    "match_path = Path(outputs, 'feats-superpoint-n4096-r1024_matches-superglue_pairs20_argo_query.h5')\n",
    "assert match_path.exists(), match_path\n",
    "match_file = h5py.File(str(match_path), 'r')\n",
    "print(list(match_file.keys())[0])\n",
    "print(match_file['query.jpg_ring_front_center_315978410361853528.jpg']['matches0'].shape)\n",
    "print(match_file['query.jpg_ring_front_center_315978410361853528.jpg']['matching_scores0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n",
      "110001\n"
     ]
    }
   ],
   "source": [
    "from hloc.utils.read_write_model import read_images_binary, read_points3d_binary\n",
    "images = read_images_binary(sfm_dir / 'models/model_aligned/images.bin')\n",
    "points3D = read_points3d_binary(sfm_dir / 'models/model_aligned/points3D.bin')\n",
    "print(len(list(images.keys())))\n",
    "print(len(list(points3D.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[209, 230, 231, 232, 233, 262, 272, 291] \n",
      "\n",
      "[396, 446]\n",
      "(1416, 2)\n",
      "(1416,)\n",
      "[81045 81133]\n",
      "[[2616.50577184 1213.18699374   19.25434028]\n",
      " [2616.47963398 1213.33981656   18.85064648]] \n",
      "\n",
      "[448]\n",
      "(1467, 2)\n",
      "(1467,)\n",
      "[81133]\n",
      "[[2616.47963398 1213.33981656   18.85064648]] \n",
      "\n",
      "[445]\n",
      "(1453, 2)\n",
      "(1453,)\n",
      "[81133]\n",
      "[[2616.47963398 1213.33981656   18.85064648]] \n",
      "\n",
      "[]\n",
      "(1490, 2)\n",
      "(1490,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[386, 456]\n",
      "(1503, 2)\n",
      "(1503,)\n",
      "[109578  81133]\n",
      "[[2616.15898121 1213.6670485    19.25946615]\n",
      " [2616.47963398 1213.33981656   18.85064648]] \n",
      "\n",
      "[368, 413]\n",
      "(1506, 2)\n",
      "(1506,)\n",
      "[ 80929 109420]\n",
      "[[2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[335, 397]\n",
      "(1507, 2)\n",
      "(1507,)\n",
      "[ 80929 109420]\n",
      "[[2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[274, 269, 270, 314]\n",
      "(1412, 2)\n",
      "(1412,)\n",
      "[ 80929  80930  80931 109420]\n",
      "[[2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[238, 277, 279, 280, 281, 308, 335]\n",
      "(1486, 2)\n",
      "(1486,)\n",
      "[ 81153 105043  80929  80930  80931  80936 109420]\n",
      "[[2616.16020236 1213.38443736   19.42479696]\n",
      " [2616.42933808 1213.32871811   19.23679404]\n",
      " [2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.26072465 1213.33853344   19.02877437]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[209, 230, 231, 232, 233, 262, 272, 291]\n",
      "(1508, 2)\n",
      "(1508,)\n",
      "[ 81153     -1  80929  80930  80931  81046  80936 109420]\n",
      "[[2616.16020236 1213.38443736   19.42479696]\n",
      " [2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.34370134 1212.90612367   19.2012746 ]\n",
      " [2616.26072465 1213.33853344   19.02877437]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[201, 224, 230, 231, 232, 250, 278, 283]\n",
      "(1503, 2)\n",
      "(1503,)\n",
      "[ 81153 105043  80929  80930  80931  81046  80936 109420]\n",
      "[[2616.16020236 1213.38443736   19.42479696]\n",
      " [2616.42933808 1213.32871811   19.23679404]\n",
      " [2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.34370134 1212.90612367   19.2012746 ]\n",
      " [2616.26072465 1213.33853344   19.02877437]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[197, 222, 228, 229, 230, 235, 269, 278]\n",
      "(1507, 2)\n",
      "(1507,)\n",
      "[ 81153 105043  80929  80930  80931  81046  80936 109420]\n",
      "[[2616.16020236 1213.38443736   19.42479696]\n",
      " [2616.42933808 1213.32871811   19.23679404]\n",
      " [2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.34370134 1212.90612367   19.2012746 ]\n",
      " [2616.26072465 1213.33853344   19.02877437]\n",
      " [2616.46106569 1213.31044779   18.91367291]] \n",
      "\n",
      "[173, 219, 213, 214, 215, 226, 267]\n",
      "(1514, 2)\n",
      "(1514,)\n",
      "[81153 80933 80929 80930 80931 81046 80936]\n",
      "[[2616.16020236 1213.38443736   19.42479696]\n",
      " [2616.89827661 1215.50696616   18.79861439]\n",
      " [2616.19847176 1213.13001356   19.27126524]\n",
      " [2616.18767883 1213.37673094   19.24748593]\n",
      " [2616.13562578 1213.45941652   19.24777382]\n",
      " [2616.34370134 1212.90612367   19.2012746 ]\n",
      " [2616.26072465 1213.33853344   19.02877437]] \n",
      "\n",
      "[]\n",
      "(1525, 2)\n",
      "(1525,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[208]\n",
      "(1581, 2)\n",
      "(1581,)\n",
      "[80562]\n",
      "[[2603.93536962 1167.70112608   27.75273992]] \n",
      "\n",
      "[211]\n",
      "(1558, 2)\n",
      "(1558,)\n",
      "[80562]\n",
      "[[2603.93536962 1167.70112608   27.75273992]] \n",
      "\n",
      "[]\n",
      "(1565, 2)\n",
      "(1565,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[]\n",
      "(1655, 2)\n",
      "(1655,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[]\n",
      "(1662, 2)\n",
      "(1662,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[]\n",
      "(1703, 2)\n",
      "(1703,)\n",
      "[]\n",
      "[] \n",
      "\n",
      "[[2616.49270291 1213.26340515   19.05249338]\n",
      " [2616.47963398 1213.33981656   18.85064648]\n",
      " [2616.47963398 1213.33981656   18.85064648]\n",
      " [2616.3193076  1213.50343253   19.05505632]\n",
      " [2616.32976873 1213.22023067   19.09246907]\n",
      " [2616.32976873 1213.22023067   19.09246907]\n",
      " [2616.24571052 1213.3191522    19.17004947]\n",
      " [2616.26187245 1213.33261396   19.19579475]\n",
      " [2616.24963863 1213.27224333   19.19072055]\n",
      " [2616.27210106 1213.27930267   19.19647973]\n",
      " [2616.27210106 1213.27930267   19.19647973]\n",
      " [2616.31209733 1213.58603167   19.17428362]\n",
      " [2603.93536962 1167.70112608   27.75273992]\n",
      " [2603.93536962 1167.70112608   27.75273992]]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False]\n",
      "[[2616.49270291 1213.26340515   19.05249338]\n",
      " [2616.47963398 1213.33981656   18.85064648]\n",
      " [2616.47963398 1213.33981656   18.85064648]\n",
      " [2616.3193076  1213.50343253   19.05505632]\n",
      " [2616.32976873 1213.22023067   19.09246907]\n",
      " [2616.32976873 1213.22023067   19.09246907]\n",
      " [2616.24571052 1213.3191522    19.17004947]\n",
      " [2616.26187245 1213.33261396   19.19579475]\n",
      " [2616.24963863 1213.27224333   19.19072055]\n",
      " [2616.27210106 1213.27930267   19.19647973]\n",
      " [2616.27210106 1213.27930267   19.19647973]\n",
      " [2616.31209733 1213.58603167   19.17428362]]\n",
      "[2616.33702808 1213.32963155   19.09313239]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox = [80, 275, 55, 75] # [x,y,width,height]\n",
    "\n",
    "def bbox_filter(bbox, query_keypoints):\n",
    "    filtered_ids = []\n",
    "    for i in range(query_keypoints.shape[0]):\n",
    "        pos = query_keypoints[i]\n",
    "        if pos[0] > bbox[0] and pos[0] < bbox[0]+bbox[2] and \\\n",
    "           pos[1] > bbox[1] and pos[1] < bbox[1]+bbox[3]:\n",
    "            filtered_ids.append(i)\n",
    "    return filtered_ids\n",
    "\n",
    "filtered_ids = bbox_filter(bbox, feature_file_q['query.jpg']['keypoints'])\n",
    "print(filtered_ids, '\\n')\n",
    "\n",
    "mean_pos_list = []\n",
    "for match_name in list(match_file.keys()):\n",
    "    db_img_name = match_name.split('jpg_')[-1]\n",
    "    matches = match_file[match_name]['matches0']\n",
    "    filtered_matches = [id for id in matches[filtered_ids] if id != -1]\n",
    "    print(filtered_matches)\n",
    "    \n",
    "    db_img_id = list(feature_file.keys()).index(db_img_name)\n",
    "    check_name = (images[db_img_id+1].name == db_img_name) # TODO: why need to + 1\n",
    "    \n",
    "    if check_name:\n",
    "        keypoints = images[db_img_id+1].xys\n",
    "        p3ids = images[db_img_id+1].point3D_ids\n",
    "        print(keypoints.shape)\n",
    "        print(p3ids.shape)\n",
    "        selected_p3id3 = p3ids[filtered_matches]\n",
    "        print(selected_p3id3)\n",
    "        p3D = np.array([points3D[j].xyz for j in selected_p3id3 if j != -1])\n",
    "        print(p3D, '\\n')\n",
    "        if p3D.size != 0:\n",
    "            mean_pos = np.mean(p3D, axis = 0)\n",
    "#             print(mean_pos, '\\n')\n",
    "            mean_pos_list.append(mean_pos)\n",
    "\n",
    "mean_pos_arr = np.array(mean_pos_list)\n",
    "print(mean_pos_arr)\n",
    "def reject_outliers(data, m=2):\n",
    "    return abs(data - np.mean(data)) < m * np.std(data)\n",
    "mask = reject_outliers(mean_pos_arr[:, 2])\n",
    "print(mask)\n",
    "mean_pos_arr_filtered = mean_pos_arr[mask, :]\n",
    "# print(mean_pos_arr_filtered)\n",
    "print(np.mean(mean_pos_arr_filtered, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hloc",
   "language": "python",
   "name": "hloc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
